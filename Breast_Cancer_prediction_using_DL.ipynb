{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLyuGg2Z66moFGm7UBZPQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hubertmeteor/Breast-cancer-prediction-using-Deep-Learning/blob/main/Breast_Cancer_prediction_using_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessaory Models for Deep Learning**\n",
        "\n",
        "Imports the main PyTorch library.......**import torch**\n",
        "\n",
        "Imports the neural network module from PyTorch, which provides tools for building neural networks..........**import torch.nn as nn**\n",
        "\n",
        "\n",
        " Imports the optimization module from PyTorch, which provides various optimization algorithms for training neural networks.......**import torch.optim as optim**\n",
        "\n",
        "Imports the load_breast_cancer function from scikit-learn, which allows you to load the Breast Cancer Wisconsin dataset........**from sklearn.datasets import load_breast_cancer**\n",
        "\n",
        "Imports the train_test_split function from scikit-learn, which allows you to split datasets into training and testing subsets.......**from sklearn.model_selection import train_test_split**\n",
        "\n",
        "\n",
        "Imports the StandardScaler class from scikit-learn, which allows you to standardize features by removing the mean and scaling to unit variance.......**from sklearn.preprocessing import StandardScaler**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KMsMqET8-fkU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMOKHD_s7_Fb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Device Configuration Testing**"
      ],
      "metadata": {
        "id": "0wQUVPF8__Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available()else \"cpu\")\n",
        "print(f\"Using device:{device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRVBEz5A__av",
        "outputId": "7a1c5382-178f-4ac3-cac1-a4a06c9c2dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection and preprocessing**"
      ],
      "metadata": {
        "id": "5Mg93JaoAu1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=load_breast_cancer()\n",
        "X=data.data\n",
        "y=data.target"
      ],
      "metadata": {
        "id": "f7yxj5EmAvkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCcZGWGBBMwR",
        "outputId": "23974e78-5ade-4814-fe32-0df013c91dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
            " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
            " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
            " ...\n",
            " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
            " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
            " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfbCqJeeBRLx",
        "outputId": "8671222f-c448-49ac-b41d-a5cf07510fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSDAoONRBVJ2",
        "outputId": "bd5f9ef4-e5b4-4226-9e59-ac9a1bca0814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data splited as training dataset(80%) and test dataset (20%)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "OY_TdGImBmQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrvV_42ACDvp",
        "outputId": "7549fd04-f764-4063-db82-0f44ea638659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n",
            "(455, 30)\n",
            "(114, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This preprocessing step is commonly used in machine learning workflows to scale features such that they have zero mean and unit variance, which can help improve the performance of many machine learning algorithms."
      ],
      "metadata": {
        "id": "_pNLV_vFDkRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()                   #This instance will be used to scale the features of your dataset.\n",
        "X_train=scaler.fit_transform(X_train)     #the mean and standard deviation of the training data will be computed and used to scale the features.\n",
        "X_test=scaler.transform(X_test)           #fitted to the training data to transform the test data (X_test).\n",
        "                                          #This ensures that the same scaling is applied to both the training and test data."
      ],
      "metadata": {
        "id": "RgybGCAWDBZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy1Yu3kaEdLa",
        "outputId": "3ae282f8-dde8-41c5-daf0-8ef5978bac60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepares the data for training and testing with PyTorch models. Make sure that the device variable is properly defined, typically either as \"cuda\" for GPU or \"cpu\" for CPU, before executing these lines."
      ],
      "metadata": {
        "id": "TtJXTNfVFknb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
        "y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
        "X_test=torch.tensor(X_test,dtype=torch.float32).to(device)\n",
        "y_test=torch.tensor(y_test,dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "SuAJTLBdEsQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Architecture**"
      ],
      "metadata": {
        "id": "FqQE8bkIF0Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):  #method initializes the neural network layers\n",
        "    super(NeuralNet,self).__init__()                      #defines a fully connected layer\n",
        "    self.fc1=nn.Linear(input_size,hidden_size)            #fc1--->fully Connected layer one\n",
        "    self.relu=nn.ReLU()                                   #creates a rectified linear unit activation function, commonly known as ReLU.\n",
        "    self.fc2=nn.Linear(hidden_size,output_size)           ##fc2--->fully Connected layer second\n",
        "    self.sigmoid=nn.Sigmoid()                             #creates a sigmoid activation function, typically used for binary classification problems to squash the output into the range [0, 1].\n",
        "\n",
        "  def forward(self,x):\n",
        "    out=self.fc1(x)           #Passes the input x through the first fully connected layer (fc1).\n",
        "    out=self.relu(out)        #Applies the rectified linear unit activation function (relu) to the output of the first layer.\n",
        "    out=self.fc2(out)         #Passes the output of the first layer through the second fully connected layer (fc2).\n",
        "    out=self.sigmoid(out)     #Applies the sigmoid activation function (sigmoid) to the output of the second layer.\n",
        "                              #This is typically used when the network is used for binary classification tasks.\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "OF9ozBD7F8_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Hyperparameters**"
      ],
      "metadata": {
        "id": "x4zJVN2UKoOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size=X_train.shape[1]\n",
        "hidden_size=64\n",
        "output_size=1\n",
        "learning_rate=0.001\n",
        "num_epoches=100"
      ],
      "metadata": {
        "id": "Hb7Xfp2TF6yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating an instance of the NeuralNet class, passing the parameters input_size, hidden_size, and output_size, and then moving the model to the specified device using .to(device)\n",
        "model=NeuralNet(input_size,hidden_size,output_size).to(device)\n"
      ],
      "metadata": {
        "id": "LmLKn39FLXvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line creates an instance of the binary cross-entropy loss function (BCELoss) from PyTorch's nn module. BCELoss is commonly used for binary classification problems. It measures the binary cross-entropy between the target and the output.\n",
        "\n",
        "Adam is an adaptive learning rate optimization algorithm that is well-suited for training deep neural networks. It updates the parameters (weights) of the model based on the gradients of the loss function with respect to those parameters. Here, you're passing model.parameters() to specify which parameters of the model should be optimized, and lr=learning_rate sets the learning rate for the optimizer to the value you defined earlier."
      ],
      "metadata": {
        "id": "KWdMn9s4M2h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCELoss()\n",
        "optimzer=optim.Adam(model.parameters(),lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "Zz3UJ1NpL55Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Neural network**"
      ],
      "metadata": {
        "id": "ifdqxzR3NLzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**for epoch in range(num_epoches)::** This loop iterates over the specified number of epochs (num_epoches), allowing the model to train for multiple passes over the entire dataset.\n",
        "\n",
        "**model.train()::** This sets the model to training mode. This is necessary because some layers may behave differently during training (e.g., dropout layers).\n",
        "\n",
        "**optimizer.zero_grad()::** This clears the gradients of all optimized tensors. Gradients accumulate by default, so it's necessary to clear them before each optimization step.\n",
        "\n",
        "Forward Pass:\n",
        "\n",
        "**outputs = model(X_train)::** Passes the input data (X_train) through the model to get the predicted outputs.\n",
        "loss = criterion(outputs, y_train.view(-1, 1)): Computes the loss using the criterion (loss function), comparing the predicted outputs (outputs) with the actual labels (y_train).\n",
        "Backward Pass and Optimization:\n",
        "\n",
        "**loss.backward():: **Computes gradients of the loss with respect to the model parameters (backpropagation).\n",
        "**optimizer.step()::** Updates the model parameters based on the computed gradients (optimization step).\n",
        "Evaluation:\n",
        "\n",
        "**with torch.no_grad()::** This block ensures that gradients are not computed for operations inside it. This is necessary because we are only evaluating the model's performance and not updating parameters.\n",
        "predicted = outputs.round():: Rounds the predicted outputs to get binary predictions (0 or 1).\n",
        "**correct = (predicted == y_train.view(-1, 1)).float().sum()::** Computes the number of correct predictions by comparing the predicted labels with the actual labels.\n",
        "**accuracy = correct / y_train.size(0)::** Computes the accuracy by dividing the number of correct predictions by the total number of training examples."
      ],
      "metadata": {
        "id": "iVEVBzCdS2NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(num_epoches):\n",
        "  model.train()\n",
        "  optimzer.zero_grad()\n",
        "  outputs=model(X_train)\n",
        "  loss=criterion(outputs,y_train.view(-1,1))\n",
        "  loss.backward()\n",
        "  optimzer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    predicted=outputs.round()\n",
        "    correct=(predicted==y_train.view(-1,1)).float().sum()\n",
        "    accuracy=correct/y_train.size(0)\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f\"Epoch[{epoch+1}/{num_epoches}],Loss:{loss.item():.4f},Accuracy:{accuracy.item()*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhF-ROnMNCPV",
        "outputId": "5ad6a13d-9292-40c6-81b9-fcd322603e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[10/100],Loss:0.5700,Accuracy:81.32%\n",
            "Epoch[20/100],Loss:0.4481,Accuracy:88.57%\n",
            "Epoch[30/100],Loss:0.3541,Accuracy:91.65%\n",
            "Epoch[40/100],Loss:0.2830,Accuracy:93.41%\n",
            "Epoch[50/100],Loss:0.2311,Accuracy:95.16%\n",
            "Epoch[60/100],Loss:0.1937,Accuracy:96.26%\n",
            "Epoch[70/100],Loss:0.1668,Accuracy:96.70%\n",
            "Epoch[80/100],Loss:0.1470,Accuracy:97.14%\n",
            "Epoch[90/100],Loss:0.1320,Accuracy:97.14%\n",
            "Epoch[100/100],Loss:0.1202,Accuracy:97.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**\n",
        "\n",
        "Training data set and Testing data set"
      ],
      "metadata": {
        "id": "-VGRbBaQRN61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs=model(X_train)\n",
        "  predicted=outputs.round()\n",
        "  correct=(predicted==y_train.view(-1,1)).float().sum()\n",
        "  accuracy=correct/y_train.size(0)\n",
        "print(f\"Accuracy on training  data: {accuracy.item()*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OfwjrkSRQgJ",
        "outputId": "106ceb7d-eaa0-4352-aee1-6e94beb98f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training  data: 97.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs=model(X_test)\n",
        "  predicted=outputs.round()\n",
        "  correct=(predicted==y_test.view(-1,1)).float().sum()\n",
        "  accuracy=correct/y_test.size(0)\n",
        "print(f\"Accuracy on test  data: {accuracy.item()*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz2sfEYHSHmR",
        "outputId": "0e87e9c5-6717-4f44-c31a-a8cc8a102379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test  data: 96.49%\n"
          ]
        }
      ]
    }
  ]
}